{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-covering",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-strap",
   "metadata": {
    "cell_marker": "\"\"\"",
    "region_name": "md"
   },
   "source": [
    "Mostly taken from https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/cifar10-baseline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-visitor",
   "metadata": {
    "title": "Imports"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from pl_bolts.datamodules import CIFAR10DataModule\n",
    "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization\n",
    "from pytorch_lightning import LightningModule, seed_everything, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.optim.swa_utils import AveragedModel, update_bn\n",
    "from torchmetrics.functional import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-founder",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Global context"
   },
   "outputs": [],
   "source": [
    "seed_everything(7)\n",
    "\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "BATCH_SIZE = 256 if AVAIL_GPUS else 64\n",
    "NUM_WORKERS = int(os.cpu_count() / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-coast",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Data"
   },
   "outputs": [],
   "source": [
    "train_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomCrop(32, padding=4),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    cifar10_normalization(),\n",
    "])\n",
    "\n",
    "test_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    cifar10_normalization(),\n",
    "])\n",
    "\n",
    "cifar10_dm = CIFAR10DataModule(\n",
    "    data_dir='./datasets',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    num_samples=200,\n",
    "    train_transforms=train_transforms,\n",
    "    test_transforms=test_transforms,\n",
    "    val_transforms=test_transforms,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-beginning",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Model"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = torchvision.models.resnet18(pretrained=False, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-witch",
   "metadata": {
    "title": "Lightning"
   },
   "outputs": [],
   "source": [
    "\n",
    "class LitModel(LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            lr=0.05,\n",
    "            batch_size=BATCH_SIZE,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters() # The arguments are saved as hparams\n",
    "        self.model = create_model()\n",
    "\n",
    "    def forward(self, x): # Same as for a pytorch Module\n",
    "        out = self.model(x)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        self.log('train_loss', loss) # Logs metrics in tensorboard\n",
    "        return loss # Lightning takes care of opt.zero_grad(), loss.backward(), opt.step() \n",
    "\n",
    "    def evaluate(self, batch, stage=None): # NOT a lightning method\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f'{stage}_loss', loss, prog_bar=True)\n",
    "            self.log(f'{stage}_acc', acc, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, 'val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, 'test')\n",
    "\n",
    "    def configure_optimizers(self): # Configure opt and scheduler\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        steps_per_epoch = 45000 // self.hparams.batch_size\n",
    "        scheduler_dict = {\n",
    "            'scheduler': OneCycleLR(\n",
    "                optimizer,\n",
    "                0.1,\n",
    "                epochs=self.trainer.max_epochs,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "            ),\n",
    "            'interval': 'step',\n",
    "        }\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler_dict} # lightning will call opt.step() and scheduler_dict['scheduler'].step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-provision",
   "metadata": {
    "title": "Instantiate model"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = LitModel(lr=0.05)\n",
    "model.datamodule = cifar10_dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-teddy",
   "metadata": {
    "title": "Instantiate trainer"
   },
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=30,\n",
    "    gpus=AVAIL_GPUS,\n",
    "    logger=TensorBoardLogger(save_dir='lightning_logs', default_hp_metric=False),\n",
    "    callbacks=[ModelCheckpoint(filename='best.ckpt')],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-coupon",
   "metadata": {
    "title": "Train"
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, cifar10_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-attention",
   "metadata": {
    "title": "Test"
   },
   "outputs": [],
   "source": [
    "trainer.test(model, datamodule=cifar10_dm)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "region_name,title,-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
